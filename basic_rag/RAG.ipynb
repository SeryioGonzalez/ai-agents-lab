{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6238b9-a521-4c22-b1b6-11c6f248cfaa",
   "metadata": {},
   "source": [
    "# Procesado de PDFs y RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed3a8f-5ad1-406a-9123-100a4aa82685",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymupdf==1.23.22\n",
    "%pip install transformers tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df133082",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665b3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tiktoken\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c3073-c140-47fc-bac1-14fe909c23ff",
   "metadata": {},
   "source": [
    "## Nos bajamos un pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf9883-ed1a-4907-b082-7d68fd6a84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF document\n",
    "local_filename = \"human_nutrition.pdf\"\n",
    "url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "# Download PDF if it doesn't already exist\n",
    "if not os.path.exists(local_filename):\n",
    "  print(f\"File {local_filename} doesn't exist, downloading...\")\n",
    "\n",
    "  # GET request to the URL\n",
    "  response = requests.get(url)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "      # Save content\n",
    "      with open(local_filename, \"wb\") as file:\n",
    "          file.write(response.content)\n",
    "      print(f\"File saved as {local_filename}\")\n",
    "  else:\n",
    "      print(f\"Failed. Status code: {response.status_code}\")\n",
    "else:\n",
    "  print(f\"File {local_filename} exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa28d3-b497-4a7c-a288-a93d512c8ff7",
   "metadata": {},
   "source": [
    "## Exploramos el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34089398-c17c-46a9-adb8-0524971afc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
    "\n",
    "    # Other potential text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "# Only text\n",
    "def open_and_read_pdf(pdf_path: str):\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    \n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\") \n",
    "    \n",
    "    for page_number, page in enumerate(doc):  \n",
    "        text = page.get_text()  # Get plain text encoded as UTF-8\n",
    "        text = text_formatter(text)\n",
    "        \n",
    "        # Tokenize text and calculate stats\n",
    "        tokens = tokenizer.encode(text)  # Proper tokenization\n",
    "        word_count = len(text.split())  # Simple word count\n",
    "        \n",
    "        pages_and_texts.append({\n",
    "            \"page_number\": page_number,\n",
    "            \"page_word_count\": word_count,\n",
    "            \"page_token_count\": len(tokens),  # Accurate token count\n",
    "            \"text\": text\n",
    "        })\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=local_filename)\n",
    "\n",
    "print(f\"The file has {len(pages_and_texts)} pages\")\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53fb64b-70cc-4e8e-beb8-d0d3f9b6504f",
   "metadata": {},
   "source": [
    "## El archivo tiene muchas paginas, pero algunas sin texto.\n",
    "### No tiene sentido indexar paginas vacias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28485ad3-6557-4562-befe-edfc1a185b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts = [ page for page in pages_and_texts if page['page_token_count'] > 0 ]\n",
    "len(pages_and_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84cbbad-6bfa-4e64-8bac-fb8caea26dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df = pd.DataFrame(pages_and_texts)\n",
    "pages_df = pages_df.set_index(\"page_number\")\n",
    "pages_df.describe().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a28682-6c8f-4ec7-9931-7fec52742f8f",
   "metadata": {},
   "source": [
    "### Es importante analizar la distribución de tokens por pagina para ver si es necesario chunking\n",
    "El chunking es necesario si nos pasamos del maximo de tokens de los modelos de embeddings, sobre 8000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c5611c-2771-40b9-9313-31db1b42a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df['page_token_count'].hist(bins=30, alpha=0.7, label='Page Token Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78b6ef-9579-4a16-a87c-f2a046301eb2",
   "metadata": {},
   "source": [
    "## En este caso, estamos muy lejos del limite de tokens de los modelos\n",
    "Tenemos que solapar los paginas, de tal manera, que una pagina tenga un trozo de la anterior y de la siguiente, para evitar cortes de bloques <br>\n",
    "Un 20% es un valor de referencia común"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "090dffbe-8d6a-46b3-ab5b-e2268e2a7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_texts(input_texts, overlap_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Genera una lista de textos con cierto nivel de solapamiento del texto previo y posterior.\n",
    "\n",
    "    Args:\n",
    "        input_texts (list): Lista de textos (uno por cada elemento).\n",
    "        overlap_ratio (float): Proporción del texto previo y posterior a incluir (por defecto 0.2 = 20%).\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de textos combinados con solapamiento.\n",
    "    \"\"\"\n",
    "    output_texts = []\n",
    "\n",
    "    for i in range(len(input_texts)):\n",
    "        # Texto previo: tomar la mitad del solapamiento deseado del texto anterior si existe\n",
    "        previous_context = input_texts[i - 1][-int(len(input_texts[i - 1]) * overlap_ratio/2):] if i > 0 else \"\"\n",
    "        \n",
    "        # Texto posterior: tomar la mitad del solapamiento deseado del texto siguiente si existe\n",
    "        next_context = input_texts[i + 1][:int(len(input_texts[i + 1]) * overlap_ratio/2)] if i < len(input_texts) - 1 else \"\"\n",
    "        \n",
    "        # Combinar los contextos con el texto actual\n",
    "        combined_text = f\"{previous_context} {input_texts[i]} {next_context}\".strip()\n",
    "        output_texts.append(combined_text)\n",
    "\n",
    "    return output_texts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16eec4b-2113-4546-9489-e47225025c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_ratio = 0.2\n",
    "\n",
    "input_texts = [page['text'] for page in pages_and_texts]\n",
    "\n",
    "overlapped_texts = create_contextual_texts(input_texts, overlap_ratio)\n",
    "# Muestra un ejemplo\n",
    "overlapped_texts[80]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25318747-43b9-4525-8eec-4e7eb8965ddc",
   "metadata": {},
   "source": [
    "#### En este ejemplo, vemos que muchos textos tienen un numero pequeño de tokens.\n",
    "#### Concatenaremos paginas sucesivas con pocos tokens para tener textos de longitud similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5d318-722f-41fc-94c6-6e929ec308ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\") \n",
    "\n",
    "# Calculamos la longitud de tokens\n",
    "input_texts_and_tokens = [ { 'text': text, 'token_size': len(tokenizer.encode(text)) } for text in overlapped_texts ]\n",
    "\n",
    "input_texts_and_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e621b-342f-4847-bb97-1a680214fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Buscamos valores con los siguientes tokens\n",
    "max_tokens = 1000\n",
    "\n",
    "def concatenate_documents(docs, max_tokens):\n",
    "    \"\"\"\n",
    "    Concatenate consecutive runs of documents until their token size reaches max_tokens.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of documents, each with 'text' and 'token_size'.\n",
    "        max_tokens (int): Maximum token size for each concatenated group.\n",
    "\n",
    "    Returns:\n",
    "        list: A new list of concatenated documents.\n",
    "    \"\"\"\n",
    "    concatenated_docs = []\n",
    "    current_group = {\"text\": \"\", \"token_size\": 0}\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Check if adding the current document exceeds the max_tokens threshold\n",
    "        if current_group[\"token_size\"] + doc[\"token_size\"] <= max_tokens:\n",
    "            # Add the current document to the group\n",
    "            current_group[\"text\"] += (\" \" + doc[\"text\"]).strip()\n",
    "            current_group[\"token_size\"] += doc[\"token_size\"]\n",
    "        else:\n",
    "            # Add the current group to the result list\n",
    "            concatenated_docs.append(current_group)\n",
    "            # Start a new group with the current document\n",
    "            current_group = {\"text\": doc[\"text\"], \"token_size\": doc[\"token_size\"]}\n",
    "    \n",
    "    # Append the last group if it has content\n",
    "    if current_group[\"token_size\"] > 0:\n",
    "        concatenated_docs.append(current_group)\n",
    "    \n",
    "    return concatenated_docs\n",
    "\n",
    "\n",
    "concatenated_input_texts_and_tokens = concatenate_documents(input_texts_and_tokens, max_tokens)\n",
    "\n",
    "# Display result\n",
    "concatenated_input_texts_and_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997105e-a4a5-473b-bb7b-84ba35060dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Hemos reducido un documento de {len(pages_and_texts)} paginas a {len(concatenated_input_texts_and_tokens)} bloques de un maximo de {max_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c464c-979e-431e-9341-7f9ca6f2e686",
   "metadata": {},
   "source": [
    "## Analizamos ahora la nueva distribucion de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69034a-ac24-4311-9d61-63aafbed241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sizes = [entry[\"token_size\"] for entry in concatenated_input_texts_and_tokens]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "axes[0].hist(pages_df['page_token_count'], bins=30, range=(0, 1000), edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title(\"Histogram of Token Sizes before optimization\")\n",
    "axes[0].set_xlabel(\"Token Size\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1].hist(token_sizes, bins=30, edgecolor='black', range=(0, 1000), alpha=0.7)\n",
    "axes[1].set_title(\"Histogram of Token Sizes after optimization\")\n",
    "axes[1].set_xlabel(\"Token Size\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61704c71-833a-4178-b4e2-c0f4d41118bc",
   "metadata": {},
   "source": [
    "## Lo siguiente es calcular los embeddings de cada bloque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8fb21-9097-4d40-bc16-b0f12bca309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "az_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_URL\")\n",
    "az_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "az_openai_embeddings_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Initialize the Azure OpenAI client\n",
    "az_openai_client = AzureOpenAI(\n",
    "    azure_endpoint=az_openai_endpoint,\n",
    "    api_key=az_openai_api_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "az_openai_embeddings_deployment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317a767-b540-4d11-b54e-f782c6e89f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(text, client=az_openai_client):\n",
    "    embeddings_response = client.embeddings.create(input=text,\n",
    "                model=az_openai_embeddings_deployment_name \n",
    "            )\n",
    "\n",
    "    return embeddings_response.data[0].embedding\n",
    "\n",
    "# Test with sample embedding\n",
    "vector = calculate_embeddings(\"En un lugar de la Mancha\")\n",
    "\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b3924-2ff6-450b-88e5-ecd353a20fc7",
   "metadata": {},
   "source": [
    "### Tenemos que calcular los embeddings de la lista de texto anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671bbef-445e-44cd-939f-cd7a72cf9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_embeddings = [ {'block_id': block_id, 'text': text['text'], 'embeddings': calculate_embeddings(text['text'])}   for block_id, text in enumerate(concatenated_input_texts_and_tokens)]\n",
    "text_and_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be037d3-8d52-4f36-b317-47b8f376543b",
   "metadata": {},
   "source": [
    "## Salvamos los embeddings a un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e76af-bac5-4b6a-bcd6-3539af541815",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"text_and_embeddings.json\"\n",
    "\n",
    "# Write the list to a JSON file\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(text_and_embeddings, file)\n",
    "\n",
    "print(f\"Data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9308396-08b2-4d8c-80b2-73711ecf4f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "        vec1 (np.array): First vector.\n",
    "        vec2 (np.array): Second vector.\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity score.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def find_most_similar(input_text_embedding, data):\n",
    "    \"\"\"\n",
    "    Find the text most similar to the input embedding.\n",
    "\n",
    "    Args:\n",
    "        input_text_embedding (list or np.array): Embedding for the input text.\n",
    "        data (list): List of dictionaries with 'text' and 'embeddings' fields.\n",
    "\n",
    "    Returns:\n",
    "        list: List of texts with their similarity scores, sorted by similarity.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for entry in data:\n",
    "        similarity = cosine_similarity(input_text_embedding, np.array(entry[\"embeddings\"]))\n",
    "        similarities.append((entry[\"text\"], similarity))\n",
    "\n",
    "    # Sort by similarity (highest first)\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# Example data\n",
    "\n",
    "# Example input text embedding\n",
    "input_text_embedding = calculate_embeddings(\"Consumo recomendado de proteinas\")\n",
    "\n",
    "# Find most similar texts\n",
    "most_similar_results = find_most_similar(input_text_embedding, text_and_embeddings)\n",
    "\n",
    "# Display results\n",
    "for text, similarity in most_similar_results[:2]:\n",
    "    print(f\"Text: {text}, Similarity: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683715aa-8a33-48ea-828b-8c7e139ef302",
   "metadata": {},
   "source": [
    "## Ahora podemos crear un RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9b8c2-7eb9-4e9c-87d5-9c681b88aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert of human nutrition. \n",
    "Provide feedback based on the context provided to the user questions.\n",
    "Limit your responses to the context provided.\n",
    "Be brief in your responses, citing sources. Ideally in 4 or 5 sentences if that number delivers a complete answer.\n",
    "Respond in the same language as the user question\n",
    "\"\"\"\n",
    "\n",
    "user_question = \"Cantidad maxima de grasa saturada diaria en adultos\"\n",
    "user_question_embedding = calculate_embeddings(user_question)\n",
    "\n",
    "most_similar_results_for_context = find_most_similar(user_question_embedding, text_and_embeddings)\n",
    "\n",
    "most_similar_results_for_context[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a965e-c871-44d1-a945-a4890820b00e",
   "metadata": {},
   "source": [
    "## Por curiosidad, ploteamos el score de similarities de los diferentes resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752400ca-edd9-4490-b6df-65501446156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = [text_and_score_tuple[1] for text_and_score_tuple in most_similar_results_for_context]\n",
    "\n",
    "\n",
    "# Plot the values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(1, len(similarity_scores) +1), similarity_scores, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Bar Plot of Values\")\n",
    "plt.xlabel(\"Documents\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels for better readability\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f9585-08db-48e8-93cb-186bd39b641c",
   "metadata": {},
   "source": [
    "## Ya tenemos el contexto adecuado para la pregunta\n",
    "### Llamamos a Azure OpenAI con grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83cf3a-a7ab-4a4b-9117-db1bc47b96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_in_context = 1\n",
    "\n",
    "context = ' '.join([text for text, score in most_similar_results_for_context[0:messages_in_context]])\n",
    "\n",
    "messages  = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT },\n",
    "    {\"role\": \"user\",   \"content\": f\"QUESTION: {user_question} - CONTEXT: {context}\"}\n",
    "]\n",
    "\n",
    "az_openai_completions_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "response = az_openai_client.chat.completions.create(\n",
    "            model=az_openai_completions_deployment_name,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
