{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2bbe88c-9535-4418-a3ca-9063da9345dc",
   "metadata": {},
   "source": [
    "# RAG con Azure AI Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb95aa9-e481-4072-aa90-204165a14b44",
   "metadata": {},
   "source": [
    "# Librerias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd96f0e-5359-425a-829e-d2d828663f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from math import ceil\n",
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20188940",
   "metadata": {},
   "source": [
    "# Variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b79fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "az_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_URL\")\n",
    "az_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "az_openai_embeddings_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME\")\n",
    "\n",
    "az_search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT_URL\")\n",
    "az_search_index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "az_search_api_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "\n",
    "# Initialize the Azure OpenAI client\n",
    "az_openai_client = AzureOpenAI(\n",
    "    azure_endpoint=az_openai_endpoint,\n",
    "    api_key=az_openai_api_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "dataset_file = \"recetas_de_la_abuela_dataset.json\"\n",
    "dataset_records = 1000\n",
    "\n",
    "output_file = \"merged_output.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68ccee-86e2-4ad0-bdfc-ebd1633d81b3",
   "metadata": {},
   "source": [
    "# Descargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4635193b-8d42-44a5-b4c1-c6e51934ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download if not already downloaded\n",
    "if not os.path.exists(dataset_file):\n",
    "    # Load the dataset from Hugging Face\n",
    "    dataset = load_dataset(\"somosnlp/RecetasDeLaAbuela\", \"version_1\")\n",
    "    \n",
    "    # Take only the train split\n",
    "    dataset_train = dataset[\"train\"]\n",
    "    \n",
    "    # Option A: Use select(range(...)) to get the first records\n",
    "    dataset_train_small = dataset_train.select(range(dataset_records))\n",
    "    \n",
    "    # Convert each sample to a dictionary\n",
    "    dataset_train_list = [dict(sample) for sample in dataset_train_small]\n",
    "\n",
    "    # Save as a proper JSON array\n",
    "    with open(dataset_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset_train_list, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Dataset with {dataset_records} records saved to {dataset_file}\")\n",
    "else:\n",
    "    print(f\"Dataset already downloaded as {dataset_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db806e-7198-480e-9865-1b5c52fe86d4",
   "metadata": {},
   "source": [
    "# Cargamos el dataset en un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb7d59-3af6-4817-87be-00bd83e3e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(dataset_file)\n",
    "\n",
    "# Quick overview of the dataset\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c657d5-bc89-4363-9beb-2320409d6ca1",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8abe9b-5ec2-49d1-80e2-1786a14182dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_summary = df.isnull().sum()\n",
    "\n",
    "missing_values_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7bad2-41fb-4334-bde2-fba5f9459bcd",
   "metadata": {},
   "source": [
    "## Seleccionamos y renombramos columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\"Id\", \"Nombre\", \"URL\", \"Ingredientes\", \"Pasos\", \"Valor nutricional\"]\n",
    "\n",
    "df_subset = df[selected_columns].rename(columns=str.lower)\n",
    "# Release the source dataframe from memory\n",
    "del df\n",
    "df_subset.columns = df_subset.columns.str.replace(\" \", \"_\", regex=False)\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88304337",
   "metadata": {},
   "source": [
    "## Procesamos columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd76bcd-56cb-4530-a2ba-5700d146e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['numero_ingredientes'] = df_subset['ingredientes'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)\n",
    "\n",
    "# id pasa a ser string\n",
    "df_subset['id'] = df_subset['id'].astype(str)\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045befc1",
   "metadata": {},
   "source": [
    "## Vectorizamos campos de interes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dbb5dde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m         vectorize_column_in_chunks(df, field)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll columns processed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m run_vectorization(\u001b[43mdf_subset\u001b[49m, df_fields_to_vectorize\u001b[38;5;241m=\u001b[39mfields_to_vectorize)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_subset' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "TMP_DIR = \"tmp\"  # Directory to store embedding files\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "fields_to_vectorize = [\"nombre\", \"pasos\"]\n",
    "vector_field_name_suffix = \"_vector\"\n",
    "\n",
    "vector_fields = [f\"{field}{vector_field_name_suffix}\" for field in fields_to_vectorize]\n",
    "\n",
    "def batch_compute_embeddings(\n",
    "    texts, \n",
    "    client=az_openai_client, \n",
    "    model_name=az_openai_embeddings_deployment_name\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes a list of strings and returns a list of embeddings (one per string).\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(input=texts, model=model_name)\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "def vectorize_column_in_chunks(df, column_name, batch_size=BATCH_SIZE, tmp_dir=TMP_DIR):\n",
    "    \"\"\"\n",
    "    1. Check if a *completed* file for this column exists in tmp_root (e.g. tmp/{column_name}_embeddings.pkl).\n",
    "       If so, skip computing altogether and return the loaded embeddings.\n",
    "    2. If not, look for partial chunk files in tmp/{column_name}/chunk_{i}.pkl and resume from the first missing chunk.\n",
    "    3. Merge all chunk files into the completed file in tmp/ once done, and return the embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) If a fully completed embeddings file exists, skip everything:\n",
    "    completed_file = os.path.join(tmp_dir, f\"{column_name}_embeddings.pkl\")\n",
    "    if os.path.exists(completed_file):\n",
    "        print(f\"[SKIP] Found existing completed file for '{column_name}': {completed_file}\")\n",
    "        with open(completed_file, \"rb\") as f:\n",
    "            all_embeddings = pickle.load(f)\n",
    "        return all_embeddings\n",
    "\n",
    "    # 2) Otherwise, we need to generate (or resume) chunked files in tmp/{column_name}/\n",
    "    col_dir = os.path.join(tmp_dir, column_name)\n",
    "    os.makedirs(col_dir, exist_ok=True)\n",
    "    \n",
    "    # Figure out how many total chunks we need\n",
    "    n = len(df)\n",
    "    total_chunks = (n + batch_size - 1) // batch_size  # ceiling division\n",
    "\n",
    "    # Check existing chunks\n",
    "    existing_chunks = set()\n",
    "    for fname in os.listdir(col_dir):\n",
    "        if fname.startswith(\"chunk_\") and fname.endswith(\".pkl\"):\n",
    "            # e.g. \"chunk_3.pkl\" -> chunk index = 3\n",
    "            try:\n",
    "                idx_str = fname.replace(\"chunk_\", \"\").replace(\".pkl\", \"\")\n",
    "                idx = int(idx_str)\n",
    "                existing_chunks.add(idx)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    # 3) For each chunk, if we haven't computed it, compute & store it.\n",
    "    print(f\"[COMPUTE] Embeddings for '{column_name}' (missing chunks).\")\n",
    "    for i in tqdm(range(total_chunks), desc=f\"Vectorizing '{column_name}'\"):\n",
    "        if i in existing_chunks:\n",
    "            # Skip if this chunk file already exists\n",
    "            continue\n",
    "\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, n)\n",
    "        \n",
    "        batch_texts = df[column_name].iloc[start_idx:end_idx].fillna(\"\").tolist()\n",
    "        batch_embeddings = batch_compute_embeddings(batch_texts)\n",
    "\n",
    "        chunk_file = os.path.join(col_dir, f\"chunk_{i}.pkl\")\n",
    "        with open(chunk_file, \"wb\") as f:\n",
    "            pickle.dump(batch_embeddings, f)\n",
    "\n",
    "def run_vectorization(df, df_fields_to_vectorize=fields_to_vectorize):\n",
    "    for field_id, field in enumerate(df_fields_to_vectorize):\n",
    "        print(f\"\\nProcessing field {field_id + 1}/{len(df_fields_to_vectorize)}: {field}\")\n",
    "        vectorize_column_in_chunks(df, field)\n",
    "    print(\"\\nAll columns processed!\")\n",
    "\n",
    "run_vectorization(df_subset, df_fields_to_vectorize=fields_to_vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831bec94",
   "metadata": {},
   "source": [
    "## Salvamos dataset a fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ea2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_chunked(\n",
    "    df,\n",
    "    column_to_dir_map,\n",
    "    output_json,\n",
    "    batch_size=BATCH_SIZE\n",
    "):\n",
    "    \"\"\"\n",
    "    df: DataFrame with the 'regular' columns.\n",
    "    column_to_dir_map: dict of { \"vec_column_name\": \"path/to/chunked_embeddings_dir\" }\n",
    "                       Example:\n",
    "                          {\n",
    "                            \"nombre_vector\": \"tmp/nombre\",\n",
    "                            \"ingredientes_vector\": \"tmp/ingredientes\",\n",
    "                            ...\n",
    "                          }\n",
    "    output_json: File path for the final .jsonl output.\n",
    "    batch_size: Number of rows/embeddings per chunk.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    total_chunks = ceil(total_rows / batch_size)\n",
    "    \n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for chunk_idx in tqdm(range(total_chunks), desc=\"Exporting rows\"):\n",
    "            start_idx = chunk_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, total_rows)\n",
    "\n",
    "            # Slice the DataFrame; copy() to avoid locking references to the original df\n",
    "            df_chunk = df.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "            # Load each embedding chunk for this slice\n",
    "            chunk_embeddings_map = {}\n",
    "            for vec_col, col_dir in column_to_dir_map.items():\n",
    "                chunk_file = os.path.join(col_dir, f\"chunk_{chunk_idx}.pkl\")\n",
    "                if not os.path.exists(chunk_file):\n",
    "                    raise FileNotFoundError(f\"Chunk file not found: {chunk_file}\")\n",
    "\n",
    "                with open(chunk_file, \"rb\") as emb_f:\n",
    "                    embeddings_chunk = pickle.load(emb_f)\n",
    "                \n",
    "                chunk_embeddings_map[vec_col] = embeddings_chunk\n",
    "\n",
    "            # Convert df_chunk to a list of dicts (records)\n",
    "            records = df_chunk.to_dict(\"records\")\n",
    "\n",
    "            # local_offset is the index within this chunk\n",
    "            local_offset = 0\n",
    "            for row_dict in records:\n",
    "                # Attach embedding vectors\n",
    "                for vec_col, emb_list in chunk_embeddings_map.items():\n",
    "                    row_dict[vec_col] = emb_list[local_offset]\n",
    "                local_offset += 1\n",
    "\n",
    "                # Write JSON line\n",
    "                f_out.write(json.dumps(row_dict, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            # Release memory for this chunk\n",
    "            del df_chunk\n",
    "            del chunk_embeddings_map\n",
    "            del records\n",
    "\n",
    "            # (Optional) Collect garbage every few chunks\n",
    "            if chunk_idx % 10 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "    print(f\"Finished exporting to {output_json}\")\n",
    "\n",
    "\n",
    "vector_column_and_files = { folder_name + vector_field_name_suffix: os.path.join(TMP_DIR, folder_name)\n",
    "        for folder_name in os.listdir(TMP_DIR)\n",
    "        if folder_name in fields_to_vectorize\n",
    "}\n",
    "print(f\"Embeddings in {vector_column_and_files}\")\n",
    "create_json_chunked(df_subset, vector_column_and_files, output_file, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe41c8",
   "metadata": {},
   "source": [
    "## Cargamos datos en Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4fa1226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from merged_output.jsonl...\n",
      "Uploading 5000 documents in batches of 300...\n",
      "Uploading batch 1...\n",
      "Uploading batch 2...\n",
      "Uploading batch 3...\n",
      "Uploading batch 4...\n",
      "Uploading batch 5...\n",
      "Uploading batch 6...\n",
      "Uploading batch 7...\n",
      "Uploading batch 8...\n",
      "Uploading batch 9...\n",
      "Uploading batch 10...\n",
      "Uploading batch 11...\n",
      "Uploading batch 12...\n",
      "Uploading batch 13...\n",
      "Uploading batch 14...\n"
     ]
    },
    {
     "ename": "HttpResponseError",
     "evalue": "() Storage quota has been exceeded for this service. You must either delete documents first, or use a higher SKU for additional quota.\nCode: \nMessage: Storage quota has been exceeded for this service. You must either delete documents first, or use a higher SKU for additional quota.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m dataset \u001b[38;5;241m=\u001b[39m documents[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 17\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msearch_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/azure/search/documents/_search_client.py:580\u001b[0m, in \u001b[0;36mSearchClient.upload_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m batch\u001b[38;5;241m.\u001b[39madd_upload_actions(documents)\n\u001b[1;32m    579\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_client_headers(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 580\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[IndexingResult], results)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/azure/search/documents/_search_client.py:679\u001b[0m, in \u001b[0;36mSearchClient.index_documents\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;129m@distributed_trace\u001b[39m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindex_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: IndexDocumentsBatch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[IndexingResult]:\n\u001b[1;32m    670\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Specify a document operations to perform as a batch.\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \n\u001b[1;32m    672\u001b[0m \u001b[38;5;124;03m    :param batch: A batch of document operations to perform.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m    :raises ~azure.search.documents.RequestEntityTooLargeError\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_documents_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/azure/search/documents/_search_client.py:687\u001b[0m, in \u001b[0;36mSearchClient._index_documents_actions\u001b[0;34m(self, actions, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m batch \u001b[38;5;241m=\u001b[39m IndexBatch(actions\u001b[38;5;241m=\u001b[39mactions)\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     batch_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[IndexingResult], batch_response\u001b[38;5;241m.\u001b[39mresults)\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RequestEntityTooLargeError:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/azure/search/documents/_generated/operations/_documents_operations.py:1235\u001b[0m, in \u001b[0;36mDocumentsOperations.index\u001b[0;34m(self, batch, request_options, **kwargs)\u001b[0m\n\u001b[1;32m   1233\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[1;32m   1234\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[0;32m-> 1235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror)\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m   1238\u001b[0m     deserialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexDocumentsResult\u001b[39m\u001b[38;5;124m\"\u001b[39m, pipeline_response)\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: () Storage quota has been exceeded for this service. You must either delete documents first, or use a higher SKU for additional quota.\nCode: \nMessage: Storage quota has been exceeded for this service. You must either delete documents first, or use a higher SKU for additional quota."
     ]
    }
   ],
   "source": [
    "# Initialize client\n",
    "search_client = SearchClient(endpoint=az_search_endpoint, index_name=az_search_index_name, credential=AzureKeyCredential(az_search_api_key))\n",
    "\n",
    "# Load the documents from a jsonl file\n",
    "print(f\"Loading documents from {output_file}...\")\n",
    "documents = []\n",
    "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        documents.append(json.loads(line))\n",
    "\n",
    "# Upload the documents to the index in blocks\n",
    "batch_size = 300\n",
    "print(f\"Uploading {len(documents)} documents in batches of {batch_size}...\")\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    print(f\"Uploading batch {i // batch_size + 1}...\")\n",
    "    dataset = documents[i:i + batch_size]\n",
    "    result = search_client.upload_documents(documents=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc305a25",
   "metadata": {},
   "source": [
    "## Buscamos en esos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENROLLADO DE CERDO RELLENO CON TACU TACU\n"
     ]
    }
   ],
   "source": [
    "def vector_search(query: str, search_client=search_client, az_openai_client=az_openai_client, embedding_model_name=az_openai_embeddings_deployment_name, max_docs=3, vector_fields=vector_fields, select_fields=[\"nombre, url, pasos, valor_nutricional\"]):\n",
    "    # Vector Search\n",
    "\n",
    "    query_embeddings = az_openai_client.embeddings.create(input=query, model=embedding_model_name).data[0].embedding\n",
    "    vector_query = VectorizedQuery(vector=query_embeddings, k_nearest_neighbors=max_docs, fields=','.join(vector_fields))\n",
    "\n",
    "    results = search_client.search(  \n",
    "            search_text=None,  \n",
    "            vector_queries= [vector_query],\n",
    "            select=select_fields,\n",
    "            top=max_docs\n",
    "        ) \n",
    "\n",
    "    return results\n",
    "\n",
    "query = \"lomo de cerdo\"\n",
    "results = vector_search(query, max_docs=1)\n",
    "\n",
    "for result in results:\n",
    "    print(result['nombre'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
